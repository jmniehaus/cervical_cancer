{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schiller test --> iodine that stains different on abnormal cells on cervix\n",
    "# hinsellman --> coloscopy using scope on cervix\n",
    "# cytology --> pap smear\n",
    "# biopsy --> biopsy \n",
    "# These indicate the RESULTS of these tests, assuming they were carried out. 1 means suspect screening, 0 means okay screening. \n",
    "\n",
    "# dx columns mean previous cervical diagnosis \n",
    "# stds_number is the sum over all std columns\n",
    "# Cant figure out what stds_n_diagnosis is, though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import miceforest as mf\n",
    "from miceforest import mean_match_default\n",
    "import seaborn as sns\n",
    "from lightgbm import LGBMClassifier\n",
    "import inspect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({\"?\": pd.NA})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename columns to be more manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = df.columns \n",
    "to_rep = {\n",
    "    \"Number\" : \"n\",\n",
    "    \"Contraceptives\" : \"bc\", \n",
    "    \"Num\" : \"n\",\n",
    "    \"-\" : \"_\",\n",
    "    \"of\" : \"\",\n",
    "    \" \" : \"_\", \n",
    "    \"(\" : \"\",\n",
    "    \")\" : \"\",\n",
    "    \"/\" : \"_\",\n",
    "    \":\" : \"_\", \n",
    "    \"__\" : \"_\"\n",
    "\n",
    "}\n",
    "\n",
    "for key, value in to_rep.items(): \n",
    "    new_names = new_names.str.replace(key, value, regex = True)\n",
    "\n",
    "new_names = new_names.str.lower()\n",
    "\n",
    "df = df.set_axis(new_names, axis = 1)\n",
    "\n",
    "df.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric, axis = 1).convert_dtypes() # convert_dtypes not working without the apply() call. Probably due to the earlier replace statement, but fiddled for an hour and no dice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifies that the count of stds is the sum over all std columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[df.columns.str.startswith(\"stds_\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all((df[df.columns[df.columns.str.startswith(\"stds_\")]].drop([\"stds_time_since_first_diagnosis\", \"stds_time_since_last_diagnosis\", \"stds_n_diagnosis\", \"stds_number\"], axis = 1).sum(axis = 1) == df.stds_number).dropna())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop n_diagnosis col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"stds_n_diagnosis\"], axis = 1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check missingness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().mean().sort_values(ascending = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop time since std diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns.values[df.columns.str.startswith(\"stds_time\")], axis = 1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const = df.nunique() == 1\n",
    "\n",
    "if any(const):\n",
    "    print(\"Deleting constant columns: {}\".format(df.columns.values[const]))\n",
    "    df.drop(df.columns.values[const], axis = 1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iud/smoking years are always >0 if you have an IUD/smoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.iud == 1) & (df.iud_years == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.smokes == 1) & (df.smokes_years == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on STD Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = df[df.columns.values[df.columns.str.startswith(\"stds_\")]].drop(\"stds_number\", axis = 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pc = PCA()\n",
    "stds_s = scaler.fit_transform(stds)\n",
    "pc.fit(stds_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = pc.explained_variance_ratio_\n",
    "eigs_cum = np.cumsum(eigs)\n",
    "ind = [i + 1 for i in range(len(eigs))]\n",
    "print(eigs_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ind, eigs)\n",
    "plt.plot(ind, eigs_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.explained_variance_[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pc.components_[0:3].T * np.sqrt(pc.explained_variance_[0:3])\n",
    "\n",
    "loadmat = pd.DataFrame(np.round(loadings, 4), columns=['PC1', 'PC2', 'PC3'], index=stds.columns.values)\n",
    "loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop([\"smokes\", \"hormonal_bc\", \"iud\", \"stds\", \"schiller\", \"biopsy\", \"hinselmann\"], axis = 1)\n",
    "x[\"n_stds\"] = x[\"stds_number\"]\n",
    "x.drop(x.columns.values[x.columns.str.startswith(\"stds\")], axis = 1, inplace = True)\n",
    "y = df[[\"biopsy\"]].astype(\"int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x.select_dtypes(include=['Int64', 'Float64']).columns.values] = x.select_dtypes(include=['Int64', 'Float64']).astype('float')\n",
    "x[x.columns.values[x.columns.str.startswith(\"dx\")]] = x[x.columns.values[x.columns.str.startswith(\"dx\")]].astype(\"category\")\n",
    "x.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, y_test.shape)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_grid_template = {\n",
    "    # \"boosting\" : \"gbdt\",\n",
    "    \"objective\" : \"poisson\"\n",
    "    # \"num_iterations\" : (25, 1000),\n",
    "    # \"max_depth\" : (1, 10),\n",
    "    # \"num_leaves\" : (4, 25),\n",
    "    # \"min_data_in_leaf\" : (1, 15),\n",
    "    # \"min_sum_hessian_in_leaf\" : (0, .1),\n",
    "    # \"min_gain_to_split\" : (0, .1),\n",
    "    # \"bagging_fraction\" : (.1, 1),\n",
    "    # \"feature_fraction\" : 1,\n",
    "    # \"feature_fraction_bynode\" : (.5, 1),\n",
    "    # \"learning_rate\" : (1e-5, .1),\n",
    "    # \"cat_smooth\" : (0, 25)\n",
    "}\n",
    "\n",
    "impute_grid = {}\n",
    "keys = x.columns.values[x.isna().any()] \n",
    "\n",
    "for i in keys: \n",
    "\n",
    "    impute_grid[i] = impute_grid_template.copy()\n",
    "\n",
    "    if \"dx\" in i: \n",
    "        impute_grid[i][\"objective\"] = \"binary\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_parameters, losses = impute_kernel.tune_parameters(\n",
    "#   #variables = keys,\n",
    "#   variable_parameters = impute_grid,\n",
    "#   dataset = 0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed = impute_kernel.complete_data(dataset=0)[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miss = x_train[keys]\n",
    "\n",
    "# fig, axes = plt.subplots(2,4,figsize=(12, 6))\n",
    "# for i, ax in enumerate(axes.ravel()):\n",
    "#     miss.plot.hist(column = miss.columns.values[i], ax = ax, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_match = mean_match_default.copy()\n",
    "mean_match.set_mean_match_candidates(5)\n",
    "\n",
    "impute_kernel = mf.ImputationKernel(x_train, mean_match_scheme=mean_match, datasets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "class mice_imputer():\n",
    "    \"\"\"\n",
    "    Wrapper class for miceimputer around sklearn transformers to avoid error in miceimputer which requires the transform method to be called on the same dataset as the fit method was. This is a problem when trying to fit on a training set and \n",
    "    transform on a validation set within an sklearn pipeline that is called within gridsearchCV. \n",
    "\n",
    "    Pass any arguments as kwargs to this class from miceimputer's ImputationKernel() class, as well as from the ImputationKernel.tune_parameters() method. Appropriate fit and transform methods will then be created such that the miceimputer.trasform\n",
    "    method will work on new data. \n",
    "\n",
    "    Note: miceimputers randomsearch tuning will be performed whenever fit() is called. Tuning miceimputer through an sklearn parameter grid would otherwise be a bit \n",
    "    \"\"\"\n",
    "    def __init__(self, variable_parameters = None, **kwargs):\n",
    "        self.all_kwargs = kwargs\n",
    "        self.lgb_args = {\"num_iterations\", \"learning_rate\", \"num_leaves\", \n",
    "                         \"max_depth\", \"min_data_in_leaf\", \"min_sum_hessian_in_leaf\", \n",
    "                         \"bagging_fraction\", \"colsample_bytree\", \"colsample_bynode\", \n",
    "                         \"lambda_l1\", \"lambda_l2\", \"min_split_gain\", \"cat_smooth\"}\n",
    "        self.lgb_args = self.__arg_intersect(self.all_kwargs, self.lgb_args, right_fn = False)\n",
    "        self.inst_args = self.__arg_intersect(self.all_kwargs, mf.ImputationKernel)\n",
    "        self.mice_args = self.__arg_intersect(self.all_kwargs, mf.ImputationKernel.mice)\n",
    "        self.variable_parameters = variable_parameters\n",
    "        #self.__map_dict(self.lgb_args)\n",
    "        #self.__map_dict(self.inst_args)\n",
    "        #self.__map_dict(self.mice_args)\n",
    "        self.kern = []\n",
    "\n",
    "\n",
    "        self.invalid = set(self.all_kwargs.keys()).difference(set(self.inst_args.keys()).union(set(self.lgb_args.keys()), set(self.mice_args.keys())))\n",
    "\n",
    "        if len(self.invalid) > 0: \n",
    "            warnings.warn(\"Invalid **kwargs will be ignored:{}\".format(self.invalid))\n",
    "\n",
    "    def __arg_intersect(self, kwargs_dict, right, right_fn = True):\n",
    "        right = inspect.getfullargspec(right).args if right_fn else right\n",
    "        inter = kwargs_dict.keys() & right \n",
    "        out_dict = {key: kwargs_dict[key] for key in inter}\n",
    "\n",
    "        return out_dict \n",
    "    \n",
    "    def __warn_clean(message, category, filename, lineno, file = None, line = None):\n",
    "        return (\"%s:%s %s: %s\\n\") % (filename, lineno, category.__name__, message) \n",
    "    \n",
    "    warnings.formatwarning = __warn_clean\n",
    "\n",
    "    # def __map_dict(self, dict):\n",
    "    #     for k, v in dict.items():\n",
    "    #         setattr(self, k, v)\n",
    "\n",
    "    def __merge_dict(self, *args):\n",
    "        base = dict()\n",
    "        for i in args:\n",
    "            base.update(i)\n",
    "        return(base)\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_params(self, deep = True):\n",
    "        return self.__merge_dict(self.lgb_args, self.inst_args, self.mice_args)\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "        \n",
    "\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        self.kern = mf.ImputationKernel(X, save_models = 2, **self.inst_args)\n",
    "        self.kern.mice(variable_parameters = self.variable_parameters, **self.mice_args, **self.lgb_args)\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        return self.kern.impute_new_data(X, copy_data = True).complete_data(inplace = False)\n",
    "        \n",
    "    def fit_transform(self, X, y = None):\n",
    "        return self.fit(X).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(7, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"imputer\", mice_imputer()),\n",
    "    (\"classifier\", clf) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\"imputer__num_iterations\" : [1, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = grid, \n",
    "    scoring = \"recall\",\n",
    "    refit = True, \n",
    "    cv = folds,\n",
    "    return_train_score = True,\n",
    "    n_jobs = 1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv.fit(x_train, y_train.values.flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cervical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36a4d8ebaadf09211e7ea3eda12f34cb65c6aa2c0258225f56eef2ad8727d175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
